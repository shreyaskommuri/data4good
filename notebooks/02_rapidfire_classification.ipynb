{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20883a07",
   "metadata": {},
   "source": [
    "# Phase 2: RapidFire AI Classification\n",
    "\n",
    "This notebook demonstrates the NLP-based job title classification system:\n",
    "\n",
    "1. **Rule-based classifier** - Fast keyword matching baseline\n",
    "2. **LLM classifier** - GPT-based classification for nuanced cases\n",
    "3. **RapidFire framework** - Parallel testing of multiple prompts/models\n",
    "4. **Evaluation pipeline** - Comprehensive metrics and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from src.models.job_classifier import (\n",
    "    RuleBasedClassifier, \n",
    "    LLMClassifier,\n",
    "    EnsembleClassifier,\n",
    "    ClimateCategory\n",
    ")\n",
    "from src.models.rapidfire import (\n",
    "    RapidFireEngine, \n",
    "    DataSharder,\n",
    "    ExperimentConfig,\n",
    "    ShardStrategy,\n",
    "    CLASSIFICATION_PROMPTS\n",
    ")\n",
    "from src.models.training_data import (\n",
    "    get_benchmark_dataset,\n",
    "    get_training_df,\n",
    "    CLIMATE_SENSITIVE_JOBS,\n",
    "    CLIMATE_RESILIENT_JOBS\n",
    ")\n",
    "from src.models.evaluation import (\n",
    "    ClassifierEvaluator,\n",
    "    MetricsCalculator\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb67a5",
   "metadata": {},
   "source": [
    "## 1. Explore Training Data\n",
    "\n",
    "View the labeled job titles used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = get_training_df()\n",
    "\n",
    "print(f\"Total labeled titles: {len(df)}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "print(f\"\\nIndustry distribution:\")\n",
    "print(df['industry'].value_counts().head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804eb835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample climate-sensitive jobs\n",
    "print(\"ðŸŒŠ Climate-Sensitive Jobs:\")\n",
    "for job in CLIMATE_SENSITIVE_JOBS[:8]:\n",
    "    print(f\"  - {job.title} ({job.industry})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample climate-resilient jobs\n",
    "print(\"ðŸ’» Climate-Resilient Jobs:\")\n",
    "for job in CLIMATE_RESILIENT_JOBS[:8]:\n",
    "    print(f\"  - {job.title} ({job.industry})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e0f49",
   "metadata": {},
   "source": [
    "## 2. Rule-Based Classification\n",
    "\n",
    "Test the fast keyword-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rule-based classifier\n",
    "rule_classifier = RuleBasedClassifier(confidence_threshold=0.6)\n",
    "\n",
    "# Test on sample titles\n",
    "test_titles = [\n",
    "    \"Surf Instructor\",\n",
    "    \"Software Engineer\",\n",
    "    \"Commercial Fisherman\",\n",
    "    \"Remote Project Manager\",\n",
    "    \"Beach Lifeguard\",\n",
    "    \"Data Scientist\",\n",
    "    \"Restaurant Server\",  # Ambiguous\n",
    "    \"Uber Driver\"         # Ambiguous\n",
    "]\n",
    "\n",
    "print(\"Rule-Based Classification Results:\\n\")\n",
    "print(f\"{'Title':<25} {'Category':<20} {'Confidence':<10} {'Keywords'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for title in test_titles:\n",
    "    result = rule_classifier.classify(title)\n",
    "    keywords = \", \".join(result.matched_keywords[:3])\n",
    "    print(f\"{title:<25} {result.category.value:<20} {result.confidence:.2f}       {keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758668d",
   "metadata": {},
   "source": [
    "## 3. Evaluate Rule-Based Classifier\n",
    "\n",
    "Test on the benchmark dataset with full metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f84c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark dataset\n",
    "bench_titles, bench_labels = get_benchmark_dataset()\n",
    "print(f\"Benchmark dataset: {len(bench_titles)} samples\")\n",
    "print(f\"  Climate-sensitive: {bench_labels.count('climate_sensitive')}\")\n",
    "print(f\"  Climate-resilient: {bench_labels.count('climate_resilient')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluator = ClassifierEvaluator()\n",
    "result = evaluator.evaluate(\n",
    "    classifier=rule_classifier,\n",
    "    test_titles=bench_titles,\n",
    "    ground_truth=bench_labels,\n",
    "    model_name=\"rule_based\",\n",
    "    config_name=\"threshold_0.6\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Metrics:\")\n",
    "print(f\"  Accuracy:   {result.metrics.accuracy:.3f}\")\n",
    "print(f\"  Precision:  {result.metrics.precision:.3f}\")\n",
    "print(f\"  Recall:     {result.metrics.recall:.3f}\")\n",
    "print(f\"  F1 Score:   {result.metrics.f1_score:.3f}\")\n",
    "print(f\"  Avg Conf:   {result.metrics.avg_confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show misclassifications\n",
    "if result.misclassified_titles:\n",
    "    print(\"\\nâŒ Misclassified Titles:\")\n",
    "    for i, title in enumerate(result.misclassified_titles[:10]):\n",
    "        idx = result.misclassified_indices[i]\n",
    "        pred = result.predictions[idx]\n",
    "        true = result.ground_truth[idx]\n",
    "        print(f\"  '{title}': predicted={pred}, true={true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5bbdb",
   "metadata": {},
   "source": [
    "## 4. Compare Different Thresholds\n",
    "\n",
    "Find the optimal confidence threshold for the rule-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifiers with different thresholds\n",
    "thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "classifiers = {\n",
    "    f\"threshold_{t}\": RuleBasedClassifier(confidence_threshold=t)\n",
    "    for t in thresholds\n",
    "}\n",
    "\n",
    "# Compare\n",
    "comparison = evaluator.compare_classifiers(\n",
    "    classifiers=classifiers,\n",
    "    test_titles=bench_titles,\n",
    "    ground_truth=bench_labels\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Threshold Comparison:\")\n",
    "print(comparison[['model', 'accuracy', 'precision', 'recall', 'f1_score', 'uncertain_count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best threshold\n",
    "best_idx = comparison['f1_score'].idxmax()\n",
    "best_threshold = comparison.loc[best_idx, 'model']\n",
    "best_f1 = comparison.loc[best_idx, 'f1_score']\n",
    "\n",
    "print(f\"\\nâœ… Best Threshold: {best_threshold} (F1={best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796da58",
   "metadata": {},
   "source": [
    "## 5. RapidFire Framework Demo\n",
    "\n",
    "Use the hyper-parallelization framework to test multiple configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available prompt templates\n",
    "print(\"ðŸ“ Available Prompt Templates:\\n\")\n",
    "for name, template in CLASSIFICATION_PROMPTS.items():\n",
    "    preview = template[:100].replace('\\n', ' ')\n",
    "    print(f\"  {name}: {preview}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5de9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RapidFire engine\n",
    "engine = RapidFireEngine(\n",
    "    num_shards=2,\n",
    "    max_workers=2\n",
    ")\n",
    "\n",
    "# Add experiments with different thresholds\n",
    "for threshold in [0.5, 0.6, 0.7]:\n",
    "    config = ExperimentConfig(\n",
    "        name=f\"rule_thresh_{threshold}\",\n",
    "        prompt_template=\"N/A\",\n",
    "        model_name=\"rule_based\",\n",
    "        temperature=threshold,  # Using temperature field for threshold\n",
    "        metadata={\"confidence_threshold\": threshold}\n",
    "    )\n",
    "    engine.add_experiment(config)\n",
    "\n",
    "print(f\"Added {len(engine.experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098454fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier factory and classify function\n",
    "def classifier_factory(config: ExperimentConfig):\n",
    "    threshold = config.metadata.get(\"confidence_threshold\", 0.6)\n",
    "    return RuleBasedClassifier(confidence_threshold=threshold)\n",
    "\n",
    "def classify_func(classifier, title):\n",
    "    result = classifier.classify(title)\n",
    "    return result.to_dict()\n",
    "\n",
    "# Run all experiments\n",
    "results_df = engine.run_all(\n",
    "    data=bench_titles,\n",
    "    classifier_factory=classifier_factory,\n",
    "    classify_func=classify_func,\n",
    "    ground_truth=bench_labels\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ RapidFire Results:\")\n",
    "print(results_df[['config_name', 'accuracy', 'f1_score', 'total_time']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best experiment\n",
    "best = engine.get_best_experiment(metric=\"f1_score\")\n",
    "if best:\n",
    "    print(f\"\\nâœ… Best Experiment: {best.config.name}\")\n",
    "    print(f\"   Accuracy: {best.accuracy:.2%}\")\n",
    "    print(f\"   F1 Score: {best.f1_score:.2%}\")\n",
    "    print(f\"   Time: {best.total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729fccf",
   "metadata": {},
   "source": [
    "## 6. LLM Classification (Optional)\n",
    "\n",
    "If you have an OpenAI API key, test the LLM-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for OpenAI API key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "has_openai_key = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(f\"OpenAI API Key configured: {has_openai_key}\")\n",
    "\n",
    "if not has_openai_key:\n",
    "    print(\"\\nTo enable LLM classification:\")\n",
    "    print(\"1. Get an API key from https://platform.openai.com/\")\n",
    "    print(\"2. Add OPENAI_API_KEY to your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM classifier (only if API key is available)\n",
    "if has_openai_key:\n",
    "    from src.models.job_classifier import LLMClassifier, LLMClassifierConfig\n",
    "    \n",
    "    # Create LLM classifier\n",
    "    config = LLMClassifierConfig(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    llm_classifier = LLMClassifier(config)\n",
    "    \n",
    "    # Test on a few ambiguous titles\n",
    "    ambiguous_titles = [\n",
    "        \"Restaurant Server\",\n",
    "        \"Hotel Concierge\",\n",
    "        \"Uber Driver\",\n",
    "        \"Wedding Planner\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸ¤– LLM Classification Results:\")\n",
    "    for title in ambiguous_titles:\n",
    "        result = llm_classifier.classify(title)\n",
    "        print(f\"  {title}: {result.category.value} ({result.confidence:.2f})\")\n",
    "        print(f\"    Reasoning: {result.reasoning}\")\n",
    "else:\n",
    "    print(\"Skip LLM testing (no API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01f141",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "### Results\n",
    "- Rule-based classifier provides fast baseline classification\n",
    "- RapidFire framework enables parallel testing of configurations\n",
    "- Key metrics: Accuracy, Precision, Recall, F1 Score\n",
    "\n",
    "### Next Steps (Phase 3)\n",
    "1. Integrate classification with workforce data pipeline\n",
    "2. Build Markov chain models for job transition dynamics\n",
    "3. Correlate climate events with workforce state changes\n",
    "4. Develop resilience prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fa5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final evaluation report\n",
    "print(evaluator.generate_report())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
